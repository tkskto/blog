---
title: 'G検定勉強メモ'
publishDate: 2025-10-11
tags: ['エンジニアリング']
---

[ディープラーニングG検定（ジェネラリスト）最強の合格テキスト［第2版］](https://www.amazon.co.jp/dp/4815622752/)のメモ。

## ダートマス会議

1956年に開催された会議。主催のジョンマッカーシーが在籍していたのが、ダートマス大学だったらしい。

マービン・ミンスキー、ナサニエル・ロチェスター、クロード・シャノンなども発起人。

## AIブームの変遷

第一次は1950から1960年。明確に定義された特定の問題を解くことはできたが、実用的な利用には程遠く、おもちゃレベルだということでトイプロブレムと呼ばれている。ちなみにイライザが開発されたのは1964年。

第二次は1980年頃。エキスパートシステムと呼ばれる、専門家にヒアリングをして集めた大量の知識をもとに受け答えができるレベル。知識を詰め込むコストが膨大（知識獲得のボトルネック）だったり、例外や矛盾に柔軟に対応するようなことはできず。

エキスパートシステムとしては有機化合物にかんする推論を行うDENDRAL、感染症を扱うMYCIN、緑内障を扱うCASNETなどがある。

第三次は2000年以降、インターネットが普及し、データが爆発的に増え、ビッグデータと呼ばれるようになった。ディープラーニングなども技術も拍車をかける

## オントロジー

> オントロジー（ontology）とは、元々は哲学の世界で言う「存在論（存在とは何かを研究する学問）」を示す単語ですが、ITの世界では知識の共有化や再利用の方法として研究開発が進み「対象世界をどの様に捉えた（概念化した）かを記述するもの」という意味で使われています。

引用: [オントロジー（用語解説） :: NTTデータ バリュー・エンジニア-Webサイト](https://www.nttdata-value.co.jp/glossary/ontology)

「人がどのように情報をとらえて理解しているか」ということ自体をコンピュータに理解させるにはどうすればいいか、という考え方。言葉の意味、言葉と言葉の関係性を他人と共有できるように明確なルールに基づいて定義することを重視する。

オントロジーにはヘビーウェイトと、ライトウェイトがある。

*   ライトウェイト: 用語の定義と階層を中心に、最低限の関係だけを持つ。「語彙の共有」「タグ付け・ナビゲーション・検索」をしやすくするための軽量なスキーマ。
    *   「動物 > 犬 > 柴犬」。is-aやpart-ofの関係を定義する
*   ヘビーウェイト: 用語の定義に加え、性質、属性、関係、制約を形式的に記述して機械推論できるレベルまで厳密化したもの。整合性チェックや新しい知識の導出を狙う。
    *   「犬と猫は同じものではない」「人が犬を飼っていれば、その人は飼い主」のような

## 中国語の部屋

チューリングテストに対する有名な反論らしい。チューリングテストに合格したとしても、知能があるとは言えないよね、ということ。

中が見えない部屋から中国語が聞こえてきたとしてもその人は中国語を知っているとは限らない（マニュアルを使って話しているだけで、知識がある、理解しているとは限らない）

## シンボルグラウンディング問題

人間が当たり前にできる「言葉などの記号（シンボル）と、それが指し示す現実世界の対象や概念とを結びつけて意味を理解すること」が、AIにとって難しいという課題。

言葉を言葉で説明するだけだと循環するので、知覚や行為との結びつきが必要。この問題を解決するには、身体性を通じて得られる感覚や経験が必要らしい。

## フレーム問題

問題解決において「何が関係があり、何が無視すべきことか」を自律的に判断できないという、AI研究における本質的な課題。

持ち合わせる知識をすべて使うと、情報量が膨大すぎてフリーズしてしまう。

## 強いAIと弱いAI

弱いAIは特定の仕事に特化したAI。画像認識や翻訳など、決められた範囲で動く。

強いAIは人間のように理解し、さまざまな課題を自律的に解けるAI。一般的な知能（AGI）に近い考え方で、今はまだ実現していない。

現状は、世の中で使われているのはほとんどが弱いAI。ChatGPTなどの大規模言語モデルも弱いAIらしい。言語を中心に多用途だが、人の指示の範囲で動く弱いAI。

強いAIと呼べる目安の例:

*   自律性: 自分で目標を立て、計画して行動できる。
*   一般性: 未知の課題にも柔軟に転用できる。
*   世界との結びつき: 感覚や行為を通じて現実に働きかけ、結果から学べる。

## 機械学習技術の活用イメージ

機械学習はインターネットの普及により、ビッグデータの貯蓄が簡単になった（材料）こと、ハードウェアの性能が向上したこと、アルゴリズムが改善したことで、爆発的に普及し始めた。

### 回帰予測

- 数値をそのまま予測する。
- 例: 売上、在庫切れまでの日数、到着時間、気温。
- 指標: MAEやMAPEなどの誤差を見る。
- メモ: 季節性やトレンドを入れると安定しやすい。ベースライン（平均・前週同曜日）と比較する。

### 分類

- データにラベルを割り振る。
- 例: スパム/非スパム、解約する/しない、不良/良品。
- 指標: 精度だけでなく、適合率・再現率・F1も見る。閾値の調整でバランスを取る。
- メモ: ラベルの偏りが大きい時は重み付けやサンプリングを使う。

### 異常検知

- 「普段と違う」を見つける。ラベルが少ない/ない状況でも使える。
- 例: 機械の振動の異常、アクセスログの不審な動き、売上の急落。
- 方法: 統計的手法、自己符号化器、孤立森林など。
- メモ: アラート数を管理するために、しきい値と通知の運用を先に決める。

## 機械学習の分類

### 教師あり学習

入力と正解（ラベル）の対応を学ぶ。教師あり学習で扱うタスクは主に分類問題と回帰問題。

### 教師なし学習

正解なしでデータの構造や特徴を探すもの。クラスタリングや次元圧縮を使う。用途はセグメンテーション、特徴量作成、異常検知の前処理など。結果の解釈は人が行い、クラスタ数の決め方に工夫がいる。

### 半教師あり学習

正解ラベルがついているデータとついていないデータの両方を使う。

正解ラベル付きのデータを十分に用意できない場合や、正解ラベルをつけるコストを削減したい場合などに使う。

### 強化学習

環境で行動し、報酬が最大になるよう学ぶ。状態・行動・報酬・方策が基本要素。

ゲーム、ロボット、在庫や配車の最適化、広告入札などで使われる。

## 特徴量エンジニアリング

機械学習モデルの学習または予測の精度を高めるために、データを加工し、特徴量を作成すること。

*   予測変数として採用する列を選別する
*   データに前処理を施し、学習・予測に効果的な形に加工する
    *   変換（「生年月日」を現時点「年齢」に変換しておくとか、文字列データを数値データに変換するとか、フラグを0と1にするとか）
    *   欠損値の処理
    *   正規化や標準化（平均が0、標準偏差が1になるように変換すること）

男性を1、女性を0というようデータ自体の大小という性質を持たないデータをカテゴリカルデータという。

春夏秋冬をデータとしてもつ「季節」という列を4列のフラグ列にし、各行について1つの列のみ「1」になるような前処理をOne Hotエンコーディングという。

たいして春夏秋冬にたいして1,2,3,4のように割り振ることをラベルエンコーディングという。

## 欠損値の扱い方

*   リストワイズ法：データをそのまま削除する。総データ量が多い場合に使いやすい。欠陥自体になんらかの傾向がある場合に削除すると、データ自体の傾向を変化させてしまうリスクもある
*   統計量で補完：欠損になっていないデータだけを使って、平均、中央値、最頻値などを算出し補うやり方
*   回帰補完：欠損列と非欠損列に相関がある場合に、回帰を利用して、埋める。非欠損データを利用して、補充値を推測するモデルを作る。

## ノーフリーランチ定理

> あらゆる問題において、高い精度を出せる汎用的なモデルは存在しない

## 過学習

学習データを過剰に学習することで、モデルを学習データに合わせすぎてしまい、未知のデータに対する汎用性が失われてしまっている状況を指す。オーバーフィッティング、過剰適合ともいう。

*   学習データでは精度がよいのに、未知の本番データでは精度が悪くなる。
*   データ数が少ないのに、特徴量の数が多い
*   相関が多い特徴量が多い
*   モデルが複雑すぎる（高次元の関数になっている）
*   モデルのパラメータの値が大きすぎる

対策としては以下が考えられる。

*   学習データの数を増やす（偏りがない方が望ましい）
*   ハイパーパラメータを調整することで、複雑さを抑える
*   正則化を実施する

### バイアスとバリアンス

バイアスは、モデルの平均的な予測と、真の目標値（データ生成過程が与える期待値）の差のこと。モデルが単純すぎるとこの差が大きくなり、学習後も予測と正解の間の誤差（損失）が高止まりする。これがアンダーフィット。

バリアンスは、訓練データを入れ替えたときにモデルの予測がどれだけ変わるかという、ばらつきの大きさのこと。モデルが複雑すぎると、訓練データ上では損失が小さい／分類が正解するが、検証・テストデータ上では損失が大きい／誤分類が増える状態になりやすい。これがオーバーフィット。

バイアスが小さく、バリアンスが大きい状態だと過学習になりやすい。

## 線形回帰

線形回帰とは、1つ以上の説明と直線関数を使用して、連続値である目的変数を予測する手法。

### 単回帰分析

説明変数が1つだけある線形回帰のこと。

```math
Y = aX+b
```

Xが説明変数で、Yは目的変数、aとbは回帰係数という。aは目的変数Yに対する説明変数Xの影響度を表す。

例: X軸に1日の平均湿度をとり、Y軸の値に当たる洗濯物が乾くまでの時間を予測するタスクは、単回帰分析になる。

### 重回帰分析

説明変数が2つ以上ある線形回帰のこと。

```math
\hat{y}_i=a_1 x_{1,i}+a_2 x_{2,i}+...+b
```

Xiが説明変数で、Yは目的変数、aiは偏回帰係数と呼ばれ、各説明変数の予測したい量への影響度を表しているため、偏回帰係数の大小を比較することで、どの変数が予測に重要なのかという目安を得ることができる。

例: 1日の平均湿度と風速の2つの説明変数から、洗濯物が乾くまでの時間（Y）を予測する。

相関が高い説明変数同士を特徴量として組み合わせたときに互いに干渉し精度を悪くしてしまう現象を多重共線性（Multicollinearity）という。

変数同士の相関係数を計算した結果、相関係数が1または-1に近い場合に相関が高いといいます。

## 損失関数

モデルの予測と正解の差を数値化する関数。モデルの学習とは、この関数の値が最小になるように係数（パラメータ）を調整することを指す。

損失関数は予測値と正解値を入力に取り、係数の値に依存して変わる。つまりモデルの良し悪しを数値化したもの。

重回帰分析の予測値を

```math
\hat{y}=a_1 x_1+a_2 x_2+b
```

とした時、損失関数は、

```math
E(a_1,a_2,b)=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2 \quad\text{(平均二乗誤差)}
```

のように、関数式の中に予測値を含む。実際の最適化では、上の損失 E に正則化項を足した J(ω) を最小化する。回帰では平均二乗誤差（MSE）、分類では交差エントロピー損失をよく使う。

## 正則化（Regularization）

損失関数に、係数（重み）に対してのみ罰則の項（正則化項）を加える。これは切片（オフセット値）には通常適用せず、重みが過大になるのを防ぐための仕組み。

### 損失関数と正則化の関係

学習で最小化するのは「データに対する損失」そのものではなく、「損失＋正則化項」を足し合わせた目的関数である。

```math
J(\omega)=E(\omega)+\lambda R(\omega)
```

ここで、

*   E：データに対する損失（例：平均二乗誤差 MSE）
*   R：係数の大きさに対する罰（例：L1/L2 ノルム）
*   λ：罰の強さ（正則化の重み）

Eは当て方のズレを減らし、Rはモデルの複雑さを抑える。正則化項Rは一般的には以下のような形になる。

```math
\lambda\sum_{i}|\omega_i|^p
```

### L1正則化

正則化項Rを`p=1`、つまりパラメータの絶対値の和をペナルティ項にする。不要なパラメータを落とすことで、特徴選択と次元圧縮の効果がある。

L1正則化を取り入れた線形回帰はラッソ回帰と呼ばれる。

### L2正則化

正則化項Rを`p=2`、つまりパラメータの絶対値の二乗和をペナルティ項にする。パラメータの大きさをゼロに近づける（影響を小さく抑える）ことで、汎用性の高い滑らかなモデルが得られ、過学習防止に効果的。

L2正則化を取り入れた線形回帰をリッジ回帰と呼ぶ。

ラッソ回帰とリッジ回帰を組み合わせた手法をElastic Netと呼ぶ。

## ロジスティック回帰分析

ロジスティック回帰分析はある事象が発生する確率を求める非線形回帰の手法。線形回帰と同様な考え方をクラス分類問題に応用するような手法。

分類するカテゴリが2つである二値分類と、3つ以上であるマルチクラス分類の両方に適用できる。

二値分類に使う関数はシグモイド関数（どんな数値を入力しても、出力が0から1になるようなS字型の滑らかな曲線を描く関数）であり、一般的なシグモイド関数は以下のような式で表される。

```math
f(x)=\frac{1}{1+e^{-x}}
```

※ eはオイラー数（≈ 2.71828）

0.5などの閾値を決めることで、結果が0.5未満か、0.5以上かにより分類ができる。

説明変数がn個ある場合の式は以下のようになる。

```math
f(x)=\frac{1}{1+e^{-({a_1}{x_1}+{a_2}{x_2}+...+{a_n}{x_n})}}
```

xはどんな値であっても、分母は必ず1より大きい値になるため、f(x)の値は0から1の範囲に収まることになる。

分類問題におけるロジスティック回帰は、係数の大小が予測対象への影響度を表すので比較的解釈しやすいという利点がある。
